---
title: "HW5"
author: "Vincent"
date: "2023-02-14"
output: pdf_document
---

from Section 1.6: exercises 30,33

from Section 2.1: exercises 1,3,6,9,10,11,14,17,20,21,22. 


# 1.6 

## 30. Let $V=M_{2\times 2}(F), \space W_1=${$\begin{pmatrix}a&b\\c&d\end{pmatrix}\in V:a,b,c\in F$}, $W_2=${$\begin{pmatrix}0&a\\-a&b\end{pmatrix}\in V:a,b\in F$}. Prove that $W_1$ and $W_2$ are subspaces of V, and find the dimensions of $W_1,W_2,W_1+W_2$ and $W_1\cap W_2$

**Proof:**\newline
Base on the expression, we can decompose $W_1$ into $a\begin{pmatrix}1&0\\0&1\end{pmatrix}+b\begin{pmatrix}0&1\\0&0\end{pmatrix}+c\begin{pmatrix}0&0\\1&0\end{pmatrix}$\newline
And, $W_2$ into $a\begin{pmatrix}0&1\\-1&0\end{pmatrix}+b\begin{pmatrix}0&0\\0&1\end{pmatrix}$\newline

As $a=b=c=0$, we know that $W_1$ is linearly independent then we have $dim(W_1)=3$\newline
As $a=b=0$, we know that $W_2$ is also linearly independent then we have $dim(W_2)=2$\newline\newline
If we put $W_1+W_2$ we will have $\begin{pmatrix}1&0\\0&1\end{pmatrix},\begin{pmatrix}0&1\\0&0\end{pmatrix},\begin{pmatrix}0&0\\-1&0\end{pmatrix},\begin{pmatrix}0&0\\0&1\end{pmatrix}$, because $\begin{pmatrix}0&1\\-1&0\end{pmatrix}$ can generate by $\begin{pmatrix}0&0\\1&0\end{pmatrix}$ and $\begin{pmatrix}0&1\\0&0\end{pmatrix}$.\newline
Therefore, $dim(W_1)+dim(W_2)=dim(W_1\cap W_2)=4$\newline\newline

## 33. 
## a) Let W_1 and W_2 be subspaces of a vector space V such that $V=W_1\bigoplus W_2$. If $\beta_1$ and $\beta_2$ are bases for $W_1$ and $W_2$, respectively, show that $\beta_1\cap \beta_2= \emptyset$ and $\beta_1 \cup \beta_2$ is a basis for V

**Proof: **\newline
If that $\beta_1$ and $\beta_2$ are bases of $W_1$ and $W_2$, then they are linear independent to each other. For example, $\beta_1=${$1,0$} and $\beta_2=${$0,1$}. Also, as long as both vector space are direct sum of each other, where $V=W_1\bigoplus W_2$. Then $\beta_1 \cap \beta_2 = \emptyset$ and $\beta_1\cup \beta_2$ are basis for V.\newline\newline

## b) Conversely, let $\beta_1$ and $\beta_2$ be disjoint bases for subspaces $W_1$ and $W_2$, respectively, of a vector space V. Prove that if $\beta_1 \cup \beta_2$ is a basis for V, then $V=W_1\bigoplus W_2$.

**Proof: **\newline
$\beta_1\cap \beta_2 = \emptyset$ and $span(\beta_1)+span(\beta_2)\subset W_1\cup W_2$, then we can denote that $\beta_1$ and $\beta_2$ generate $W_1, W_2$. because $\beta_1$ and $\beta_2$ $\in V$ then $V=W_1\bigoplus W_2$.\newline\newline

# 2.1

## 1. Label the following statements as true or false. In each part, V and W are finite-dimensional vector spaces (over F), and T is a function from V to W.

**a)** True\newline
**b)** False\newline
**c)** False\newline
**d)** True\newline
**e)** False\newline
**f)** False\newline
**g)** True\newline
**h)** False\newline

## 3. $T:R^2\to R^3$ defined by $T(a_1,a_2)=(a_1+a_2,0,2a_1-a_2)$

**Proof: **\newline
$T(c(a_1,a_2,a_3))=(ca_1(1,0,2)+ca_2(1,0,-1)+ca_3(0,0,0))=c(a_1(1,0,2)+a_2(1,0,-1)+a_3(0,0,0))=cT(a_1,a_2,a_3)$\newline
$T(a_1,a_2,a_3)+T(b_1,b_2,b_3)=T(a_1+b_1,a_2+b_2,a_3+b_3)=(a_1+b_1)((1,0,2)+(1,0,-1)+(0,0,0))=(a_1(1,0,2)+a_2(1,0,-1)+a_2(0,0,0))+(b_1(1,0,2)+b_2(1,0,-1)+b_3(0,0,0))=T(a_1,a_2,a_3)+T(b_1,b_2,b_3)$\newline
Therefore, T is linear\newline
Then we find that $a_1+a_2=0,0=0,2a_1-a_2=0$, then $a_1=-a_2$ and $a_1=\frac{a_2}{2}$. Base on the expression we can have that $a_1=a_2=0$ and $(a_1,a_2)=(0,0)$ Then dim of $N(T)=0$\newline
$T(1,0)=(1,0,2)$ and $T(0,1)=(1,0,-1)$ and since the set {$T(1,0),T(0,1)$}={$(1,0,2),(1,0,-1)$} is linearly independent. and dim of $R(T)=2$\newline
Therefore, since $rank(T)+nullity(T)=2+0=2=dim(R^2)$ T is one to one but not onto.\newline\newline

## 6. $T:M_{n\times n}(F)\to F$ defined by $T(A)=tr(A)$. Recall that $tr(A)=\Sigma^n_{i=1}A_{ii}$

**Proof: **\newline
$T(cA+B)=\Sigma^n_{i=1}(cA_{ii}+B_{ii})=c\Sigma^n_{i=1}A_{ii}+\Sigma^n_{i=1}B_{ii}=cT(A)+T(B)$ Then T is linear\newline
$N(T)=n\times n-1=n^2-1$ and $R(T)=${$1$}, then $nullity(T)+rank(T)=(n^2-1)+1=n^2=dim_{n_\times n}(F)$. Therefore, T is not one to one as nullity is greater then one. But T is onto.\newline\newline

## 9. In this exercise, $T:R^2\to R^2$ is a function. For each of the following parts, state why T is not linear.

**a) ** $cT(a)+dT(b)=(c+d,ca_2+db_2)$$T(ca+db)=T(c(a_1,a_2)+d(b_1,b_2))=(1,ca_2+db_2)\ne cT(a)+dT(b)$ not linear\newline\newline
**b) ** $cT(a)+dT(b)=(ca_1+db_1,(ca_2+db_2)^2)$$T(ca+db)=T(c(a_1,a_2)+d(b_1,b_2))=(ca_1+db_1,ca_2^2+db_2^2)\ne cT(a)+dT(b)$ not linear\newline\newline
**c) ** $cT(a)+dT(b)=(sin(ca_1+db_1),0)$$T(ca+db)=T(c(a_1,a_2)+d(b_1,b_2))=(csin(a_1)+dsin(b_1),0)\ne cT(a)+dT(b)$ not linear\newline\newline
**d) ** $cT(a)+dT(b)=(|ca_1+db_1|,ca_2+db_2)$$T(ca+db)=T(c(a_1,a_2)+d(b_1,b_2))=(c|a_1|+d|b_1|,ca_2+db_2)\ne cT(a)+dT(b)$ not linear\newline\newline
**e) ** $cT(a)+dT(b)=(ca_1+db_1+1,ca_2+db_2)$$T(ca+db)=T(c(a_1,a_2)+d(b_1,b_2))=(ca_1+db_1+c+d,ca_2+db_2)\ne cT(a)+dT(b)$ not linear\newline\newline

## 10. Suppose that $T:R^2\to R^2$ is linear, $T(1,0)=(1,4)$, and $T(1,1)=(2,5)$. What is $T(2,3)$? Is T one-to-one?

**Proof: **\newline
$T(2,3)=aT(1,0)+bT(1,1)=-(1,4)+3(2,5)=(5,11)$ and we can have that $c_1=-1,c_2=3$\newline
$T(1,0)=T(1,1)-T(1,0)=(2-1,5-4)=(1,1)$ and $A = \begin{pmatrix}1&1\\ 4&1\end{pmatrix}$ Then $det(A)=3$ Therefore, A is invertible and T is one to one and onto.\newline\newline

## 11. Prove that there exists a linear transformation $T:R^2\to R^3$ such that $T(1,1)=(1,0,2)$ and $T(2,3)=(1,-1,4)$. What is $T(8,11)$

**Proof: **\newline
$T(8,11)=aT(1,1)+bT(2,3)=5(1,0,2)+3(1,-1,4)=(8,-3,22)$\newline\newline


## 14. Let V and W be vector spaces and $T:V\to W$ be linear
## a) Prove that T is one to one if and only if T carries linearly independent subsets of V onto linearly independent subsets of W

**Proof: **\newline
Suppose T is one-to-one, and let S be a linearly independent subset of V. We want to show that T(S) is a linearly independent subset of W. Suppose that T(S) is linearly dependent, i.e., there exist distinct vectors $w_1, w_2, ..., w_n$ in T(S) and scalars $c_1, c_2, ..., c_n$ not all zero such that $c_1w_1 + c_2w_2 + ... + c_nw_n = 0$. Since each wi is in T(S), we can write wi = $T(v_i)$ for some $v_i$ in S. Then we have: $c_1T(v_1) + c_2T(v_2) + ... + c_nT(v_n) = T(c_1v_1 + c_2v_2 + ... + c_nv_n) = T(0) = 0$ Since T is one-to-one, this implies that $c_1v_1 + c_2v_2 + ... + c_nv_n = 0$, which contradicts the assumption that S is linearly independent. Therefore, T(S) must be linearly independent.\newline
Suppose T carries linearly independent subsets of V onto linearly independent subsets of W, and let $v_1, v_2$ be distinct vectors in V such that $T(v_1) = T(v_2)$. We want to show that $v_1 = v_2$, i.e., that T is one-to-one. Consider the set S = {$v_1, v_2$}. Since $v_1$ and $v_2$ are distinct, S is linearly independent. By assumption, $T(S) =$ {$T(v_1), T(v_2)$} is linearly independent. Therefore, we must have $c_1T(v_1) + c_2T(v_2) = 0$ only if $c_1 = c_2 = 0$. But we know that $T(v_1) = T(v_2)$, so we have: $c_1T(v_1) + c_2T(v_2) = T(c_1v_1 + c_2v_2) = 0$ This implies that $c_1v_1 + c_2v_2 = 0$, and since S is linearly independent, we must have $c_1 = c_2 = 0$. Therefore, $v_1 = v_2$, and T is one-to-one. Combining the two implications, we conclude that T is one-to-one if and only if T carries linearly independent subsets of V onto linearly independent subsets of W.


## b) Suppose that T is one-to-one and that S is a subset of V. Prove that S is linearly independent if and only if T(S) is linearly independent.

**Proof: **\newline
Suppose T is one-to-one and S is a linearly independent subset of V. We want to show that $T(S)$ is linearly independent. Suppose that T(S) is linearly dependent, i.e., there exist distinct vectors $w_1, w_2, ..., w_n$ in T(S) and scalars $c_1, c_2, ..., c_n$ not all zero such that $c_1w_1 + c_2w_2 + ... + c_nw_n = 0$. Since each wi is in T(S), we can write $w_i = T(v_i)$ for some $v_i$ in S. Then we have: $c_1T(v_1) + c_2T(v_2) + ... + c_nT(v_n) = T(c_1v_1 + c_2v_2 + ... + c_nv_n) = 0$ Since T is one-to-one, this implies that $c_1v_1 + c_2v_2 + ... + c_nv_n = 0$, which contradicts the assumption that S is linearly independent. Therefore, T(S) must be linearly independent.\newline
Suppose T is one-to-one and T(S) is linearly independent. We want to show that S is linearly independent. Suppose that S is linearly dependent, i.e., there exist distinct vectors $v_1, v_2, ..., v_n$ in S and scalars $c_1, c_2, ..., c_n$ not all zero such that $c_1v_1 + c_2v_2 + ... + c_nv_n = 0$. Then we have: $T(c_1v_1 + c_2v_2 + ... + c_nv_n) = c_1T(v_1) + c_2T(v_2) + ... + c_nT(v_n) = 0$. Since T is one-to-one and the vi are distinct, we must have $c_1T(v_1) + c_2T(v_2) + ... + c_nT(v_n) = 0$ only if $c_1 = c_2 = ... = c_n = 0$. But we know that T(S) is linearly independent, so this implies that $c_1v_1 + c_2v_2 + ... + c_nv_n = 0$ only if $c_1 = c_2 = ... = c_n = 0$. Therefore, S is linearly independent. Combining the two implications, we conclude that S is linearly independent if and only if T(S) is linearly independent, when T is a one-to-one linear transformation.

## c) Suppose $\beta =${$v_1,v_2,...,v_n$} is a basis for V and T is one-to-one and onto. Prove that $T(\beta)=${$T(v_1),T(v_2)...,T(v_n)$} is a basis for W.

**Proof: **\newline
If T is a one-to-one and onto linear transformation from V to W, and $\beta = {v1, v2, ..., vn}$ is a basis for V, then $T(\beta) =$ {$T(v1), T(v2), ..., T(vn)$} is a basis for W.

## 17. Let V and W be finite-dimensional vector spaces and $T:V\to W$ be linear
## a) Prove that if $dim(V)<dim(W)$, then T cannot be onto

**Proof: **
Suppose that T is a linear transformation from V to W and $dim(V) < dim(W)$. We will prove that T cannot be onto. Assume, for the sake of contradiction, that T is onto. Then for any w in W, there exists v in V such that $T(v) = w$. In particular, for any basis {$w_1, w_2, ..., w_d$} of W, there exist vectors $v_1, v_2, ..., v_d$ in V such that $T(v_i) = w_i$ for $i = 1, 2, ..., d$. Now consider the set {$v_1, v_2, ..., v_d$}. Since $dim(V) < dim(W)$, we have $d > dim(V)$, so this set contains more vectors than the dimension of V. Therefore, this set must be linearly dependent. That is, there exist scalars $c_1, c_2, ..., c_d$, not all zero, such that $c_1v_1 + c_2v_2 + ... + c_dv_d = 0$. Applying T to both sides, we get: $c_1T(v_1) + c_2T(v_2) + ... + c_dT(v_d) = T(c_1v_1 + c_2v_2 + ... + c_dv_d) = T(0) = 0$ But since $T(v_i) = w_i$ for $i = 1, 2, ..., d$, we have $c_1w_1 + c_2w_2 + ... + c_dw_d = 0$, which contradicts the linear independence of the basis {$w_1, w_2, ..., w_d$} of W. Therefore, our assumption that T is onto must be false, and we conclude that if $dim(V) < dim(W)$, then T cannot be onto.

## b) Prove that if $dim(V)>dim(W)$, then T cannot be one-to-one

**Proof: **\newline
Suppose that T is a linear transformation from V to W and $dim(V) > dim(W)$. We will prove that T cannot be one-to-one. Assume, for the sake of contradiction, that T is one-to-one. Then for any two distinct vectors $u, v$ in V, we have $T(u) \ne T(v)$. In particular, for any basis {$v_1, v_2, ..., v_w$} of W, we can extend it to a basis {$v_1, v_2, ..., v_w, ..., v_n$} of V, where n > w. Now consider the set {$v_1, v_2, ..., v_n$}. Since $n > w$, this set contains more vectors than the dimension of W. Therefore, this set must be linearly dependent. That is, there exist scalars $c_1, c_2, ..., c_n$, not all zero, such that $c_1v_1 + c_2v_2 + ... + c_nv_n = 0$. Without loss of generality, assume that $c_1 \ne 0$. Then we can solve for v1 in terms of the other vectors: $v_1 = (-c_2/c_1)v_2 + (-c_3/c_1)v_3 + ... + (-c_n/c_1)v_n$ Now let u be the vector $u = (-c_2/c_1)v_2 + (-c_3/c_1)v_3 + ... + (-c_n/c_1)v_n$. Then u is a non-zero vector in V, and we have $T(u) = T(v1) = 0$, since $v_1$ can be expressed as a linear combination of the other vectors. This contradicts the assumption that T is one-to-one, since $u \ne 0$ but $T(u) = 0$. Therefore, our assumption that T is one-to-one must be false, and we conclude that if $dim(V) > dim(W)$, then T cannot be one-to-one.

## 20. Let V and W be vector spaces with subspaces $V_1$ and $W_2$ respectively. If $T:V\to W$ is linear, prove that $T(V_1)$ is a subspace of W and that {$x\in V: T(x)\in W_1$} is a subspace of V.

**Proof: **\newline
To show that $T(V_1)$ is a subspace of $W$, we need to verify that it satisfies the following three conditions:

It contains the zero vector: Since $T$ is linear, $T(0) = 0$, so $0 \in T(V_1)$.
It is closed under addition: Suppose $y_1, y_2 \in T(V_1)$. Then, there exist $x_1, x_2 \in V_1$ such that $T(x_1) = y_1$ and $T(x_2) = y_2$. Since $V_1$ is a subspace of $V$, we have $x_1 + x_2 \in V_1$. Therefore, $T(x_1 + x_2) = T(x_1) + T(x_2) = y_1 + y_2$. Thus, $y_1 + y_2 \in T(V_1)$.
It is closed under scalar multiplication: Suppose $y \in T(V_1)$ and $c$ is a scalar. Then, there exists $x \in V_1$ such that $T(x) = y$. Since $V_1$ is a subspace of $V$, we have $cx \in V_1$. Therefore, $T(cx) = cT(x) = cy$. Thus, $cy \in T(V_1)$.
Therefore, $T(V_1)$ is a subspace of $W$.

To show that ${x \in V : T(x) \in W_1}$ is a subspace of $V$, we need to verify the following three conditions:

It contains the zero vector: Since $T(0) = 0 \in W_1$, we have $0 \in {x \in V : T(x) \in W_1}$.
It is closed under addition: Suppose $x_1, x_2 \in {x \in V : T(x) \in W_1}$. Then, $T(x_1), T(x_2) \in W_1$, so $T(x_1 + x_2) = T(x_1) + T(x_2) \in W_1$. Thus, $x_1 + x_2 \in {x \in V : T(x) \in W_1}$.
It is closed under scalar multiplication: Suppose $x \in {x \in V : T(x) \in W_1}$ and $c$ is a scalar. Then, $T(x) \in W_1$, so $cT(x) \in W_1$. Thus, $T(cx) = cT(x) \in W_1$. Therefore, $cx \in {x \in V : T(x) \in W_1}$.
Therefore, ${x \in V : T(x) \in W_1}$ is a subspace of $V$.

## 21. Let V be the vector space of sequences described in Example 5 of Section 1.2. Define the functions T, $U:V\to V$ by $T(a_1,a_2,...)=(a_2,a_3,...)$ and $U(a_1,a_2,...)=(0,a_1,a_2,...)$. T and U are called the left shift and right shift operators on v, respectively.
## Prove That T and U are linear

Let a,b be sequences in V, and let c be a scalar in the underlying field. Then we have:\newline
$T(c\mathbf{a} + \mathbf{b}) = T(ca_1 + b_1, ca_2 + b_2, ...) = (ca_2 + b_2, ca_3 + b_3, ...) = c(a_2,a_3,...) + (b_2,b_3,...) = cT(\mathbf{a}) + T(\mathbf{b})$\newline
Therefore, T satisfies the additivity and homogeneity properties required for a function to be linear.

Let a,b be sequences in V, and let c be a scalar in the underlying field. Then we have:\newline
$$ U(c\mathbf{a} + \mathbf{b}) = U(ca_1 + b_1, ca_2 + b_2, ...) = (0, ca_1 + b_1, ca_2 + b_2, ...) = c(0,a_1,a_2,...) + (0,b_1,b_2,...) = cU(\mathbf{a}) + U(\mathbf{b}) $$\newline
Therefore, U satisfies the additivity and homogeneity properties required for a function to be linear.

## Prove That T is onto, but not one-to-one.

Let b be an arbitrary sequence in V, and let $a = (0, b_1, b_2, ...)$. Then $T(a) = (b_1, b_2, ...) = b$. Therefore, T is onto.\newline
Consider the sequences $a = (1, 0, 0, ...)$ and $b = (0, 1, 0, ...)$. Then $T(a) = T(b) = (0, 0, ...)$, so T is not one-to-one.

## Prove That U is one to one, but not onto.

Suppose $U(a) = U(b)$ for some sequences a,b in V. Then we have $(0,a_1,a_2,...) = (0,b_1,b_2,...)$, which implies $a_1 = b_1, a_2 = b_2,$ and so on. Therefore, $a = b$, and U is one-to-one.\newline
Let b be the sequence $(1, 0, 0, ...)$, and suppose there exists a sequence a in V such that U(a) = b. Then we have $(0,a_1,a_2,...) = (1,0,0,...)$, which implies $a_1 = 0$. But then $(0,a_1,a_2,...) = (0,0,a_2,...)$, so $U(a)$ is not equal to b. Therefore, U is not onto.\newline\newline


## 22. Let $T:R^3\to R$ be linear. Show that there exist scalars a,b and c such that $T(x,y,z)=ax+by+cz$ for all $(x,y,z)\in R^3$. can you generalize this result for $T:F\to F$? State and prove an analogous result for $T:F^n\to F^m$

**Proof: **\newline
Let $T: \mathbb{R}^3 \to \mathbb{R}$ be a linear transformation. We want to show that there exist scalars $a$, $b$, and $c$ such that $T(x,y,z) = ax + by + cz$ for all $(x,y,z) \in \mathbb{R}^3$.

Since $T$ is linear, we know that $T$ can be represented by a matrix $A$ such that $T(\mathbf{x}) = A\mathbf{x}$ for all $\mathbf{x} \in \mathbb{R}^3$. Let $A = [a_1, a_2, a_3]$, where $a_1$, $a_2$, and $a_3$ are the columns of $A$. Then we have:

$T(x,y,z) = A\begin{bmatrix} x \ y \ z \end{bmatrix} = x \begin{bmatrix} a_1 \ a_2 \ a_3 \end{bmatrix} \begin{bmatrix} a_1 \ a_2 \ a_3 \end{bmatrix} + z \begin{bmatrix} a_1 \ a_2 \ a_3 \end{bmatrix} = (xa_1 + ya_2 + za_3) = (ax + by + cz)$

where $a = a_{1,1}$, $b = a_{2,1}$, and $c = a_{3,1}$.

Therefore, we have shown that there exist scalars $a$, $b$, and $c$ such that $T(x,y,z) = ax + by + cz$ for all $(x,y,z) \in \mathbb{R}^3$.

Let $T: F^n \to F^m$ be a linear transformation. We want to show that there exist matrices $A \in F^{m \times n}$ such that $T(\mathbf{x}) = A\mathbf{x}$ for all $\mathbf{x} \in F^n$.

Since $T$ is linear, we know that $T$ can be represented by a matrix $A$ such that $T(\mathbf{x}) = A\mathbf{x}$ for all $\mathbf{x} \in F^n$. Let $A = [a_1, a_2, \ldots, a_n]$, where $a_1$, $a_2$, $\ldots$, $a_n$ are the columns of $A$. Then we have:

$T(\mathbf{x}) = A\begin{bmatrix} x_1 \ x_2 \ \vdots \ x_n \end{bmatrix} = x_1 \begin{bmatrix} a_1 \ a_2 \ \vdots \ a_m \end{bmatrix} + x_2 \begin{bmatrix} a_1 \ a_2 \ \vdots \ a_m \end{bmatrix} + \cdots + x_n \begin{bmatrix} a_1 \ a_2 \ \vdots \ a_m \end{bmatrix} = (x_1 a_1 + x_2 a_2 + \cdots + x_n a_n)$







